{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise: Working with Text Data\n",
    "\n",
    "## Working with Text Data\n",
    "\n",
    "In this exercise, you will work with text data. Text data is usually represented as strings, made up of characters with variable length. This feature is clearly very different from the numeric features and we will need to process the data before we can apply our machine learning algorithms to it.\n",
    "\n",
    "## Applying Bag-of-Words to a Toy Dataset\n",
    "\n",
    "To construct a bag-of-words model based on the word counts in the respective documents, we can use the <samp>CountVectorizer</samp> class implemented in scikit-learn. As we will see in the following code section, the <samp>CountVectorizer</samp> class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9\n",
      "Vocabulary content:\n",
      " {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "#Text data and Building Vocabulary\n",
    "import numpy as np\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining the weather is sweet and one and one is two'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "count.fit(docs)\n",
    "print(\"Vocabulary size: {}\". format(len(count.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the <samp>CountVectorizer</samp> consists of the tokenization of the training data and building of the vocabulary, which we can access as the vocabulary\\_ attribute. In this case the vocabulary consists of 7 words.\n",
    "\n",
    "To create the bag-of-words representation for the training dataset, we call the <samp>transform</samp> method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words: <3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 17 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of Bag of word:\n",
      " [[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#To create the bag-of-words representation\n",
    "bag = count.transform(docs)\n",
    "#Repr returns a string containing a printable representation of an object. \n",
    "print(\"Bag of words: {}\".format(repr(bag)))\n",
    "print(\"Dense representation of Bag of word:\\n {}\". format(bag.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words representation is stored in a sparse matrix that only stores the entries that are nonzero. A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary. In the dense representation we can see that the word counts for each word are either 0 or 1. For example, the first feature at index position 0 resembles the count of the word \"and\", which only occurs in the last documents, and the word \"is\" at the index position 1 ( the 2nd feature in the document vectors) occurs in all the three sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Application: Sentiment Analysis of Movie Reviews\n",
    "\n",
    "In this part of this exercise, we will use a dataset of movie reviews from the IMDb (Internet Movie Database). This dataset contains the text of the reviews, together with a label that indicates whether a review is \"positive\" or \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Elements:\n",
      "                                               review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n"
     ]
    }
   ],
   "source": [
    "#Movie reviews Dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/movie_data.csv')\n",
    "print(\"First Elements:\\n {}\".format(df.head(3)))\n",
    "text_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "text_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will build the vocabulary and the bag-of-word representation of the <samp>text\\_train</samp>.\n",
    "\n",
    "#### Implementation Notes:\n",
    "<ul>\n",
    "    <li> Use the Class <samp>CountVectorizer</samp></li>\n",
    "    \n",
    "    <li> Build the vocabulary using the training set </li>\n",
    "    <li> Compute the bag-of-words representation of <samp>text\\_train</samp> into <samp>X\\_train</samp> </li> \n",
    "    <li> Print the shape of <samp>X\\_train</samp> using: print(\"X_train:\\n{}\".format(repr(X_train))) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x76852 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3408554 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "#Building the vocabulary and the bag of words\n",
    "# HERE YOUR CODE\n",
    "count = CountVectorizer()\n",
    "count.fit(text_train)\n",
    "X_train = count.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of <samp>X_train</samp> is $25000\\times76852$, indicating that the vocabulary contains 76,852 entries.\n",
    "\n",
    "Let's look at the vocabulary in a bit more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 76852\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00000001', '00015', '001', '007', '0079', '007s', '0080', '0083', '009', '0093638', '00am', '00o', '00pm', '00s', '01', '0148', '02']\n",
      "Features 20010 to 20030:\n",
      "['dozor', 'doña', 'doğan', 'dp', 'dpm', 'dpp', 'dq', 'dr', 'draaaaaaaawl', 'draaaaaags', 'drab', 'drablow', 'drably', 'drabness', 'drac', 'dracht', 'dracula', 'draculas', 'draft', 'drafted', 'draftee', 'draftees', 'drafthouse', 'drafting', 'drafts', 'drag', 'dragan', 'dragged', 'dragging', 'draggy']\n",
      "Every 2000th feature:\n",
      "['00', 'affiliation', 'approxiately', 'barbara', 'blobs', 'buoyancy', 'charitable', 'commentors', 'crippling', 'demotic', 'dolous', 'elysee', 'eyelid', 'follows', 'ghettos', 'gwyenths', 'hogue', 'indefinitely', 'jessie', 'kramp', 'liz', 'marketability', 'ministrations', 'naked', 'offsets', 'patently', 'poissons', 'punisher', 'reimburse', 'rosy', 'seamed', 'singing', 'splaying', 'sumatra', 'testers', 'trifunovic', 'unrolling', 'wandering', 'y2k']\n"
     ]
    }
   ],
   "source": [
    "#Let's look at the vocabulary in a bit more detail.\n",
    "feature_names = count.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20400:20430]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all numbers. All these numbers appear somewhere in the reviews, and are therefore extracted as words. \n",
    "\n",
    "Once we have our feature, let's obtain a qualitative measure of performance by actually building a classifier. We have the training labels stored in <samp>y_train</samp> and the bag-of-words representation of the training data in <samp>X_train</samp>, so we can train a classifier on this data. For high-dimensional, sparse data like this, linear models like <samp>LogisticRegression</samp> could work best. \n",
    "\n",
    "Let's start by evaluating <samp>LogisticRegression</samp>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "# HERE YOUR CODE \n",
    "#Use fit(X,y) fuction of LogisticRegression() to train your model, \n",
    "#where X is the array of your training data and y is the label \n",
    "logreg.fit(X_train,y_train)\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_test = count.transform(text_test)\n",
    "#Use score(X,y) function of LogisticRegression() to compute \n",
    "#the performance on both training and testing set \n",
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results we can see our model overfit the data. The LogisticRegression has a regularization parameter, C, which can tune (via a grid search strategy) to reduce the overfitting effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n",
      "Test score: 0.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "def grid_search(X_train,y_train,X_test,y_test,param_grid):\n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "    # HERE YOUR CODE \n",
    "    #Use fit(X,y) fuction of GridSearchCV to select parameters and train your model, \n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    # Test on the Testing set\n",
    "    #HERE YOU CODE\n",
    "    #Use score(X,y) function of GridSearchCV to compute \n",
    "    #the performance on testing set \n",
    "    print(\"Test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "    return grid\n",
    "\n",
    "grid_search(X_train, y_train, X_test, y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain an accuracy of 89%, which indicates reasonable performance for a balanced binary classification task. Note the accurcary on the test set is the same of the previous test, but now the model does not overfit.  \n",
    "\n",
    "To reduce the computation time, in this esercixe you can test just two C values (I've already done the grid search procedure  for all the cases). In a real context you need to test multiple C values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word with Multiple Appearances\n",
    "\n",
    "To clean the vocabulary from no-meaningful \"words\" we can use a simple mechanism that works quite well in practice: only use tokens that appear only at least two documents (or at least five documents, and so on). A token that appears only in a single document is unlikely to appear in the test set and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the <samp>min\\_df</samp> parameter (see below).\n",
    "\n",
    "By requiring at least five appearances of each token, we can bring down the number of feature to 27,040 - only about a third of the original features. There are clearly many fewer numbers, and some of the more obscure words seem to have vanished. \n",
    "\n",
    "The validation accuracy is unchanged from before. We did not improve our model, but having fewer features to deal with speeds up processing and throwing away useless features might make the model more interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27040 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3328371 stored elements in Compressed Sparse Row format>\n",
      "Number of features: 27040\n",
      "First 50 features:\n",
      "['00', '000', '007', '01', '02', '03', '05', '06', '07', '08', '09', '10', '100', '1000', '100s', '100th', '101', '102', '103', '104', '105', '107', '108', '109', '10s', '10th', '11', '110', '111', '115', '116', '117', '11th', '12', '120', '1200', '123', '12th', '13', '130', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '16mm']\n",
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n",
      "Test score: 0.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={'C': [0.1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set minimum number of documents a token needs to appear\n",
    "#Building the vocabulary and the bag of words\n",
    "# HERE YOUR CODE\n",
    "# min_df allow us to select all word that at least appeare more than five time\n",
    "count = CountVectorizer(min_df=5)\n",
    "count.fit(text_train)\n",
    "X_train = count.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))\n",
    "feature_names = count.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "\n",
    "# HERE YOUR CODE\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_test = count.transform(text_test)\n",
    "grid_search(X_train, y_train, X_test, y_test, param_grid) # è una modalità verbosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Tokenization\n",
    "\n",
    "The <samp>CountVectorize</samp> is relatively simple, but it could be improved using external methods.\n",
    "One particular step that is often improved in more sophisticated text-processing applications is the first step in the bag-of-words model: tokenization. This step defines what constitutes a word for the purpose of feature extraction. \n",
    "We saw earlier that the vocabulary often contains singular and plural version of some words: \"drawback\" and \"drabacks\" or \"dracula\" and \"draculas\". For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only increase overfitting, and not allow the model to fully exploit the training data. \n",
    "\n",
    "This problem can be overcome by representing each word using its <samp>word stem</samp>, which involves identifying all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as <samp>stemming</samp>. If instead a dictionary of known word is used, and the role of the word is the sentence is taken into account, the process is referred to as <samp>lemmatization</samp> and the standardized form of the word is referred to as the <samp>lemma</samp>.\n",
    "However, <samp>lemmatization</samp> is computationally more difficult and expensive compared to <samp>stemming</samp> and it could have little impact on the performance. \n",
    "The Natural Language Toolkit for Python (NLTK, http://www.nltk.org) implements the <samp>Snowball</samp> stemming algorithm, which we will use in the following code section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Advanced Tokenization\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenizer_snowballStemmer(text):\n",
    "    return [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_snowballStemmer(\"runners like running and thus they run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <samp>Snowball</samp> stemmer from the <samp>nltk</samp> package, we can classify the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nltk: (25000, 34756)\n",
      "Best cross-validation score: 0.88\n",
      "Best parameters:  {'C': 0.1}\n",
      "Test score: 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={'C': [0.1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification with Tokenizer NLTK\n",
    "nltk_count = CountVectorizer(tokenizer=tokenizer_snowballStemmer, min_df=5).fit(text_train)\n",
    "\n",
    "# HERE YOUR CODE\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_train_nltk = nltk_count.transform(text_train)\n",
    "print(\"X_train_nltk: {}\".format(X_train_nltk.shape))\n",
    "\n",
    "# HERE YOUR CODE\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_test_nltk = nltk_count.transform(text_test)\n",
    "grid_search(X_train_nltk,y_train,X_test_nltk,y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly talk about another useful topic called <samp>stop-word removal</samp>. Stop-words are simply those words that are extremely common in all sorts of texts and likely bear no (or only little) useful information that can be used to distinguish between different classes of documents. Example of stop-words are <i>is</i>, <i>and</i>, <i>has</i> etc. \n",
    "\n",
    "To remove stop-word from the movie review, we will use the set of 127 English stop-words that is available from the NLTK library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gabriele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing stop.words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_snowballStemmer(\"a runner like running and run a lot\") [-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this list in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nltk: (25000, 34620)\n",
      "Best cross-validation score: 0.87\n",
      "Best parameters:  {'C': 0.1}\n",
      "Test score: 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={'C': [0.1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification with Tokenizer NLTK + Stop-words\n",
    "nltk_count = CountVectorizer(tokenizer=tokenizer_snowballStemmer, stop_words=stop, min_df=5).fit(text_train)\n",
    "\n",
    "# HERE YOUR CODE\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_train_nltk = nltk_count.transform(text_train)\n",
    "print(\"X_train_nltk: {}\".format(X_train_nltk.shape))\n",
    "\n",
    "# HERE YOUR CODE\n",
    "# Compute the Bag of word representation of the testing set\n",
    "X_test_nltk = nltk_count.transform(text_test)\n",
    "grid_search(X_train_nltk,y_train,X_test_nltk,y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Rescaling the Data with tf-idf\n",
    "\n",
    "One of the most common approach to represent text is using <i>term frequency-inverse document frequency</i> (tf-idf) method. The intuition of this method is to give high weight to any term that appears often in a particular document, but not in many documents in the corpus. If a word appears often in a particular document, but not in very many documents, it is likely to be very descriptive of the content of that document. <i>scikit-learn</i> implements the tf-idf method in a class: <samp>TfidfVectorizer</samp>, which takes in the text data and does both the bag-of-words feature extraction and the tf-idf transformation. There are several variants of the tf-idf rescaling schema (see wikipedia). The tf-idf score for word $w$ in document $d$ as implemented in <samp>TfidfVectorizer</samp> class is given by: \n",
    "\\begin{equation}\n",
    " tfidf(w,d) = tf * \\left(\\ln\\left( \\frac{N+1}{N_w+1}\\right)+1\\right)\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the training set, $N_w$ is the number of documents in the training set that the word $w$ appears, and $tf$ (the term frequency) is the number of times that the word $w$ appears in the query document $d$ (the document you want to transform or encode). The class also applies L2 normalization after computing the tf-idf representation; in other words, it rescales the representation of each document to have Euclidean length (this simply means each row is divided by its sum of squared entries). Rescaling in this way means that the length of a document (the number of words) does not change the vectorized representation. Test it completing the function <samp>tf_id_example</samp> using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9\n",
      "Vocabulary content:\n",
      " {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "Bag of words: <3x9 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 17 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of Bag of word:\n",
      " [[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
      "  0.43370786 0.         0.        ]\n",
      " [0.         0.43370786 0.         0.         0.         0.55847784\n",
      "  0.43370786 0.         0.55847784]\n",
      " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
      "  0.29671753 0.25119322 0.19103892]]\n"
     ]
    }
   ],
   "source": [
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining the weather is sweet and one and one is two'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count = TfidfVectorizer()\n",
    "count.fit(docs)\n",
    "print(\"Vocabulary size: {}\". format(len(count.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count.vocabulary_))\n",
    "\n",
    "#To create the bag-of-words representation\n",
    "bag = count.transform(docs)\n",
    "print(\"Bag of words: {}\".format(repr(bag)))\n",
    "print(\"Dense representation of Bag of word:\\n {}\". format(bag.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will adapt this code for the movie reviews dataset. \n",
    "Keep in mind that the tf-idf scaling is meant to find words that distinguish documents, but it is a purely unsupervised technique. So, \"important\" here does not necessarily relate to the \"positive review\" and \"negative review\" label we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x34620 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 2439240 stored elements in Compressed Sparse Row format>\n",
      "Best cross-validation score: 0.88\n",
      "Best parameters:  {'C': 10}\n",
      "Test score: 0.89\n"
     ]
    }
   ],
   "source": [
    "#Building the vocabulary and the TF-IDF bag of words\n",
    "countTFIDF = TfidfVectorizer(min_df=5,tokenizer=tokenizer_snowballStemmer, stop_words=stop,).fit(text_train)\n",
    "#HERE YOUR CODE\n",
    "XTFIDF_train = countTFIDF.transform(text_train)\n",
    "XTFIDF_test = countTFIDF.transform(text_test)\n",
    "print(\"X_train:\\n{}\".format(repr(XTFIDF_train)))\n",
    "\n",
    "grid=grid_search(XTFIDF_train, y_train, XTFIDF_test, y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Model Coefficients\n",
    "\n",
    "Finally, let's look in a bit more detail into what our logistic regression model actually learned from the data. Because there are so many features we clearly cannot look at all of the coefficients at the same time. \n",
    "However, we can look at the largest coefficients, and see which words these correspond to. \n",
    "\n",
    "The following bar char show the largest and smallest coefficients of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features wtih lowest idf:\n",
      "['/><br' 'movi' 'one' 'film' 'like' 'make' 'see' 'get' 'veri' 'watch'\n",
      " 'even' 'good' 'onli' 'would' 'time' 'realli' 'charact' 'stori' 'much'\n",
      " 'look' 'go' 'becaus' 'think' 'first' 'could' 'also' 'great' 'ani' 'peopl'\n",
      " 'scene' 'made' 'love' 'play' '/>the' 'thing' 'act' 'seem' 'bad' 'know'\n",
      " 'end' 'want' 'mani' 'come' 'way' 'take' 'never' 'show' 'say' 'it.' 'well'\n",
      " 'give' 'two' 'tri' 'littl' 'movie.' 'seen' 'doe' 'ever' 'best' 'find'\n",
      " 'plot' 'work' 'still' 'actor' '-' 'better' 'use' 'year' 'film.' 'feel'\n",
      " 'actual' 'someth' 'lot' '<br' 'part' 'back' 'whi' 'movie,' 'real' 'film,'\n",
      " \"i'm\" 'everi' 'perform' 'anoth' 'enjoy' 'interest' '/>i' 'man' 'noth'\n",
      " 'director' 'turn' 'quit' 'life' \"can't\" 'befor' 'start' 'new' 'got'\n",
      " 'live' 'thought']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show coefficients\n",
    "feature_names = np.array(countTFIDF.get_feature_names())\n",
    "sorted_by_idf = np.argsort(countTFIDF.idf_)\n",
    "print(\"Features wtih lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))\n",
    "\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "mglearn.tools.visualize_coefficients(\n",
    "        grid.best_estimator_.coef_,\n",
    "        feature_names, n_top_features=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative coefficients on the left belong to words that according to the model are indicative of negative reviews, while the positive coefficients on the right belong to words that according to the model indicate positive reviews. Most of the terms are quite intuitive, like \"worst\", \"bad\" indicating bad movie reviews, while \"great\", \"enjoy\" indicate positive movies reviews. \n",
    "The <i>mglearn</i> is a library for plotting data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
